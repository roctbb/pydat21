{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python для анализа данных\n",
    "\n",
    "## Веб-скрэйпинг: переход по ссылкам, скачивание файлов\n",
    "\n",
    "*Автор: Татьяна Рогович, НИУ ВШЭ*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На прошлом занятии мы с вами работали с табличными данными. Второй очень частой задачей для скрэйпинга является автоматический переход по ссылкам. Обычно мы встречаемся с двумя сценариями: переход по нумерованным страницам (обычно это выдача поиска или некоторый упорядоченный архив документов) и переход по определенным ссылкам на странице. Сегодня будем учиться делать и то, и то.\n",
    "\n",
    "Давайте начнем с учебного примера."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задача\n",
    "\n",
    "По ссылке http://py4e-data.dr-chuck.net/known_by_Lilian.html список людей, которых знает Лилиан. Нужно найти ее 18-го друга (считаем с одного) и перейти по ссылке (там будет список людей, которых знает этот друг). Какой имя вы извлечете, если эту операцию повторить 7 раз? То есть нам нужно найти 18-го друга 6-го человека.\n",
    "\n",
    "Прежде всего изучите исходный код страницы. В каком теге лежат ссылки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"http://py4e-data.dr-chuck.net/known_by_Tigan.html\">Tigan</a>\n"
     ]
    }
   ],
   "source": [
    "# решение\n",
    "import requests \n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "friends = requests.get('http://py4e-data.dr-chuck.net/known_by_Lilian.html').text\n",
    "soup = BeautifulSoup(friends, 'lxml')\n",
    "print(soup.find_all('a')[17]) # ссылки лежат в тэге 'a', находим 18-го друга"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что нужная нам информация лежит в атрибуте href, достать текст, как мы делали раньше не поможет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://py4e-data.dr-chuck.net/known_by_Tigan.html\n"
     ]
    }
   ],
   "source": [
    "print(soup.find_all('a')[17].get('href')) # с помощью метода get достаем информацию из атрибут href"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь осталось упаковать все в цикл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://py4e-data.dr-chuck.net/known_by_Tigan.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Mickey.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Marvin.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Yago.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Daood.html\n",
      "http://py4e-data.dr-chuck.net/known_by_Jillian.html\n",
      "Ответ: Bradly\n"
     ]
    }
   ],
   "source": [
    "friends = requests.get('http://py4e-data.dr-chuck.net/known_by_Lilian.html').text\n",
    "soup = BeautifulSoup(friends, 'lxml')\n",
    "\n",
    "for ix in range(6):\n",
    "    link = soup.find_all('a')[17].get('href')\n",
    "    print(link)\n",
    "    soup = BeautifulSoup(requests.get(link).text, 'lxml')\n",
    "    \n",
    "print('Ответ: '+soup.find_all('a')[17].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Скачивание файлов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати, еще одно применение скрэйпинга, о котором мы пока не поговорили - это автоматизированное скачивание файлов. Например, вы хотите скачать научные статьи по определенному ключевому слову или прайс-листы поставщика, которые он загружает на сайт в эскеле.\n",
    "\n",
    "Давайте посмотрим, как скачивать файлы на примере pdf и заодно попробуем походить по ссылкам. Кстати, этот процесс еще часто называется spidering или crawling, потому что ваш скрипт как паучок ползет по ссылкам (отсюда и название первых поисковых роботов - spider).\n",
    "\n",
    "Давайте попробуем сохранить англоязычные summary дисертаций, защищенных в 2019 году\n",
    "\n",
    "Мы уже отредактировали фильтры и ссылка их запомнила. Позже сегодня посмотрим как можно самим заполнять такие поля с помощью Selenium.\n",
    "\n",
    "https://www.hse.ru/sci/diss/?author=&chief=&year=2019&type=1&degree_type=&council=&spec=&fulltext=yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://www.hse.ru/sci/diss/?author=&chief=&year=2019&type=1&degree_type=&council=&spec=&fulltext=yes'\n",
    "soup = BeautifulSoup(requests.get(link).text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте для начала поэкспериментируем с первым кандидатом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<a class=\"link\" href=\"https://www.hse.ru/cookie.html\" target=\"_blank\" title=\"Пройти по ссылке\">здесь</a>,\n",
       " <a class=\"link\" href=\"https://www.hse.ru/data_protection_regulation\" target=\"_blank\" title=\"Пройти по ссылке\">здесь</a>,\n",
       " <a class=\"link no-visited with-icon with-icon_flag-spb\" href=\"//spb.hse.ru/\">Санкт-Петербург</a>,\n",
       " <a class=\"link no-visited with-icon with-icon_flag-nn\" href=\"//nnov.hse.ru/\">Нижний Новгород</a>,\n",
       " <a class=\"link no-visited with-icon with-icon_flag-perm\" href=\"https://perm.hse.ru/\">Пермь</a>,\n",
       " <a class=\"link link_no-visited link_no-underline\" href=\"/en/\">EN</a>,\n",
       " <a class=\"link link_dark no-visited\" href=\"//www.hse.ru/search/search.html?simple=0&amp;searchid=2284688\">Расширенный поиск по сайту</a>,\n",
       " <a class=\"link no-visited header_breadcrumb__link\" href=\"//www.hse.ru\"><span>Национальный исследовательский университет «Высшая школа экономики»</span></a>,\n",
       " <a class=\"link\" href=\"/sci/diss/index.html?spec=&amp;chief=&amp;fulltext=yes&amp;author=&amp;council=&amp;degree_type=&amp;order=1&amp;type=1&amp;year=2019\">по имени соискателя</a>,\n",
       " <a class=\"link\" href=\"/sci/diss/index.html?spec=&amp;chief=&amp;fulltext=yes&amp;author=&amp;council=&amp;degree_type=&amp;order=2&amp;type=1&amp;year=2019\">по имени научного руководителя</a>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(soup.find_all('a', {'class':'link'})))\n",
    "soup.find_all('a', {'class':'link'})[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что ссылок очень много, а нам нужны только те, которые ведут на summary. Можно поискать их по тексту ссылки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/29/1532361365/summary_EN_final%20(3).pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/22/1528885739/Mitrofanova%20-%20summary.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/18/1543364021/Borodina%20E._Summary_181019.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/16/1239735382/%D0%9C%D0%B5%D0%BB%D0%B8%D0%BA%D1%8F%D0%BD_Summary%2016.10.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/03/1182240389/%D0%9A%D1%83%D1%87%D0%B8%D0%BD_%D1%80%D0%B5%D0%B7%D1%8E%D0%BC%D0%B5_ENG.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/15/1539883570/summary.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/11/1541034023/Resume.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/11/1535827784/kulikova_thesis%20summary_final.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/08/26/1536057698/%D0%A1%D0%B8%D0%B4%D0%BE%D1%80%D0%BE%D0%B2%D0%B0_summary.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/04/1184372726/%D0%A0%D0%B5%D0%B7%D1%8E%D0%BC%D0%B5%20%D0%BD%D0%B0%20%D0%B0%D0%BD%D0%B3%D0%BB%D0%B8%D0%B9%D1%81%D0%BA%D0%BE%D0%BC_%D0%97%D0%B0%D1%85%D0%B0%D1%80%D0%BE%D0%B2%D0%B0_04.10.2019_%D1%84%D0%B8%D0%BD%D0%B0%D0%BB.pdf\">Summary</a>\n"
     ]
    }
   ],
   "source": [
    "for link in soup.find_all('a', text='Summary'):\n",
    "    print(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И соберем ссылки в список."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/29/1532361365/summary_EN_final%20(3).pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/22/1528885739/Mitrofanova%20-%20summary.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/18/1543364021/Borodina%20E._Summary_181019.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/16/1239735382/%D0%9C%D0%B5%D0%BB%D0%B8%D0%BA%D1%8F%D0%BD_Summary%2016.10.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/03/1182240389/%D0%9A%D1%83%D1%87%D0%B8%D0%BD_%D1%80%D0%B5%D0%B7%D1%8E%D0%BC%D0%B5_ENG.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/15/1539883570/summary.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/11/1541034023/Resume.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/11/1535827784/kulikova_thesis%20summary_final.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/08/26/1536057698/%D0%A1%D0%B8%D0%B4%D0%BE%D1%80%D0%BE%D0%B2%D0%B0_summary.pdf\">Summary</a>\n",
      "<a class=\"link\" data-hse-file=\"PDF\" href=\"/data/2019/10/04/1184372726/%D0%A0%D0%B5%D0%B7%D1%8E%D0%BC%D0%B5%20%D0%BD%D0%B0%20%D0%B0%D0%BD%D0%B3%D0%BB%D0%B8%D0%B9%D1%81%D0%BA%D0%BE%D0%BC_%D0%97%D0%B0%D1%85%D0%B0%D1%80%D0%BE%D0%B2%D0%B0_04.10.2019_%D1%84%D0%B8%D0%BD%D0%B0%D0%BB.pdf\">Summary</a>\n",
      "['/data/2019/10/29/1532361365/summary_EN_final%20(3).pdf', '/data/2019/10/22/1528885739/Mitrofanova%20-%20summary.pdf', '/data/2019/10/18/1543364021/Borodina%20E._Summary_181019.pdf', '/data/2019/10/16/1239735382/%D0%9C%D0%B5%D0%BB%D0%B8%D0%BA%D1%8F%D0%BD_Summary%2016.10.pdf', '/data/2019/10/03/1182240389/%D0%9A%D1%83%D1%87%D0%B8%D0%BD_%D1%80%D0%B5%D0%B7%D1%8E%D0%BC%D0%B5_ENG.pdf', '/data/2019/10/15/1539883570/summary.pdf', '/data/2019/10/11/1541034023/Resume.pdf', '/data/2019/10/11/1535827784/kulikova_thesis%20summary_final.pdf', '/data/2019/08/26/1536057698/%D0%A1%D0%B8%D0%B4%D0%BE%D1%80%D0%BE%D0%B2%D0%B0_summary.pdf', '/data/2019/10/04/1184372726/%D0%A0%D0%B5%D0%B7%D1%8E%D0%BC%D0%B5%20%D0%BD%D0%B0%20%D0%B0%D0%BD%D0%B3%D0%BB%D0%B8%D0%B9%D1%81%D0%BA%D0%BE%D0%BC_%D0%97%D0%B0%D1%85%D0%B0%D1%80%D0%BE%D0%B2%D0%B0_04.10.2019_%D1%84%D0%B8%D0%BD%D0%B0%D0%BB.pdf']\n"
     ]
    }
   ],
   "source": [
    "links = []\n",
    "for link in soup.find_all('a', text='Summary'):\n",
    "    print(link)\n",
    "    links.append(link.get('href'))\n",
    "    \n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично. Теперь нужно придумать, откуда взять название для каждого файла. Пусть это будут фамилии авторов, давайте доберемся до них. Такую задачу мы пока не решали: будем искать тэг по тексту, а потом искать его родителя (потому что ни ячейку таблицы с именем автора, ни саму таблицу не получится уникально отсечь)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<td class=\"p0 p\" style=\"width: 200px;\">Соискатель:</td>, <td class=\"p1 v\">Белобородова Полина Михайловна</td>]\n",
      "[<td class=\"p0 p\" style=\"width: 200px;\">Соискатель:</td>, <td class=\"p1 v\"><a class=\"link\" href=\"/staff/emitrofanova\">Митрофанова Екатерина Сергеевна</a></td>, <a class=\"link\" href=\"/staff/emitrofanova\">Митрофанова Екатерина Сергеевна</a>]\n",
      "[<td class=\"p0 p\" style=\"width: 200px;\">Соискатель:</td>, <td class=\"p1 v\">Бородина Елена Николаевна</td>]\n",
      "[<td class=\"p0 p\" style=\"width: 200px;\">Соискатель:</td>, <td class=\"p1 v\"><a class=\"link\" href=\"/org/persons/205653\">Меликян Алиса Валерьевна</a></td>, <a class=\"link\" href=\"/org/persons/205653\">Меликян Алиса Валерьевна</a>]\n",
      "[<td class=\"p0 p\" style=\"width: 200px;\">Соискатель:</td>, <td class=\"p1 v\"><a class=\"link\" href=\"/org/persons/65825225\">Кучин Илья Игоревич</a></td>, <a class=\"link\" href=\"/org/persons/65825225\">Кучин Илья Игоревич</a>]\n",
      "[<td class=\"p0 p\" style=\"width: 200px;\">Соискатель:</td>, <td class=\"p1 v\"><a class=\"link\" href=\"/org/persons/56548447\">Монахов Иван Сергеевич</a></td>, <a class=\"link\" href=\"/org/persons/56548447\">Монахов Иван Сергеевич</a>]\n",
      "[<td class=\"p0 p\" style=\"width: 200px;\">Соискатель:</td>, <td class=\"p1 v\">Годин Андрей Сергеевич</td>]\n",
      "[<td class=\"p0 p\" style=\"width: 200px;\">Соискатель:</td>, <td class=\"p1 v\">Куликова Алена Александровна</td>]\n",
      "[<td class=\"p0 p\" style=\"width: 200px;\">Соискатель:</td>, <td class=\"p1 v\">Сидорова Мария Алексеевна</td>]\n",
      "[<td class=\"p0 p\" style=\"width: 200px;\">Соискатель:</td>, <td class=\"p1 v\">Захарова Олеся Викторовна</td>]\n"
     ]
    }
   ],
   "source": [
    "for author in soup.find_all('td', text='Соискатель:'):\n",
    "    print(author.parent())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Достанем фамилии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Белобородова\n",
      "Митрофанова\n",
      "Бородина\n",
      "Меликян\n",
      "Кучин\n",
      "Монахов\n",
      "Годин\n",
      "Куликова\n",
      "Сидорова\n",
      "Захарова\n",
      "['Белобородова', 'Митрофанова', 'Бородина', 'Меликян', 'Кучин', 'Монахов', 'Годин', 'Куликова', 'Сидорова', 'Захарова']\n"
     ]
    }
   ],
   "source": [
    "authors = []\n",
    "for author in soup.find_all('td', text='Соискатель:'):\n",
    "    print(author.parent()[1].get_text().split()[0])\n",
    "    authors.append(author.parent()[1].get_text().split()[0])\n",
    "    \n",
    "print(authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, что списки действительно одинаковой длины."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(links) == len(authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь давайте попробуем реализовать сохранение файла для одного автора, а потом соберем все это в цикл. Для начала \"починим\" ссылки, добавив в начало адрес домена."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://www.hse.ru/data/2019/10/29/1532361365/summary_EN_final%20(3).pdf', 'https://www.hse.ru/data/2019/10/22/1528885739/Mitrofanova%20-%20summary.pdf', 'https://www.hse.ru/data/2019/10/18/1543364021/Borodina%20E._Summary_181019.pdf', 'https://www.hse.ru/data/2019/10/16/1239735382/%D0%9C%D0%B5%D0%BB%D0%B8%D0%BA%D1%8F%D0%BD_Summary%2016.10.pdf', 'https://www.hse.ru/data/2019/10/03/1182240389/%D0%9A%D1%83%D1%87%D0%B8%D0%BD_%D1%80%D0%B5%D0%B7%D1%8E%D0%BC%D0%B5_ENG.pdf', 'https://www.hse.ru/data/2019/10/15/1539883570/summary.pdf', 'https://www.hse.ru/data/2019/10/11/1541034023/Resume.pdf', 'https://www.hse.ru/data/2019/10/11/1535827784/kulikova_thesis%20summary_final.pdf', 'https://www.hse.ru/data/2019/08/26/1536057698/%D0%A1%D0%B8%D0%B4%D0%BE%D1%80%D0%BE%D0%B2%D0%B0_summary.pdf', 'https://www.hse.ru/data/2019/10/04/1184372726/%D0%A0%D0%B5%D0%B7%D1%8E%D0%BC%D0%B5%20%D0%BD%D0%B0%20%D0%B0%D0%BD%D0%B3%D0%BB%D0%B8%D0%B9%D1%81%D0%BA%D0%BE%D0%BC_%D0%97%D0%B0%D1%85%D0%B0%D1%80%D0%BE%D0%B2%D0%B0_04.10.2019_%D1%84%D0%B8%D0%BD%D0%B0%D0%BB.pdf']\n"
     ]
    }
   ],
   "source": [
    "links = ['https://www.hse.ru' + link for link in links]\n",
    "print(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем сохранить файл. У нас все файлы pdf, будем переименовывать их фамилиями автора. Кстати, если файлы разного формата, то расширение можно узнать через атрибут headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'application/pdf'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "requests.get(links[0]).headers['content-type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = requests.get(links[0], stream=True) # потоковое чтение файла, потому pdf может быть большим и не уместиться в памяти\n",
    "if summary.headers['content-type'] == 'application/pdf': # на всякий случай делаем проверку, иначе получим битый файл\n",
    "    fh = open('test.pdf', 'wb') # wb - запись байтовой информации\n",
    "    fh.write(summary.content) # считываем туда \"содержание\" файла по ссылке\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь еще добавим имя файла по фамилии."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = requests.get(links[0], stream=True) # потоковое чтение файла, потому pdf может быть большим и не уместиться в памяти\n",
    "if summary.headers['content-type'] == 'application/pdf': # на всякий случай делаем проверку, иначе получим битый файл\n",
    "    fh = open(f'{authors[0]}.pdf', 'wb') # wb - запись байтовой информации\n",
    "    fh.write(summary.content) # считываем туда \"содержание\" файла по ссылке\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте сохраним обработку файла в функцию и соберем уже все в цикл."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pdf(idx):\n",
    "    summary = requests.get(links[idx], stream=True) # потоковое чтение файла, потому pdf может быть большим и не уместиться в памяти\n",
    "    if summary.headers['content-type'] == 'application/pdf': # на всякий случай делаем проверку, иначе получим битый файл\n",
    "        fh = open(f'{authors[idx]}.pdf', 'wb') # wb - запись байтовой информации\n",
    "        fh.write(summary.content) # считываем туда \"содержание\" файла по ссылке\n",
    "        fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = 'https://www.hse.ru/sci/diss/?author=&chief=&year=2019&type=1&degree_type=&council=&spec=&fulltext=yes'\n",
    "soup = BeautifulSoup(requests.get(link).text, 'lxml')\n",
    "\n",
    "links = []\n",
    "for link in soup.find_all('a', text='Summary'):\n",
    "    links.append('https://www.hse.ru' + link.get('href'))\n",
    "\n",
    "authors = []\n",
    "for author in soup.find_all('td', text='Соискатель:'):\n",
    "    authors.append(author.parent()[1].get_text().split()[0])\n",
    "    \n",
    "for idx in range(len(authors)):\n",
    "    get_pdf(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось! И последнее. У нас было несколько страниц, давайте по страницам тоже пройдемся. Кликните в браузере на страницу 2 по ссылке, а потом обратно на 1. Обратите внимание, что ссылка в браузере поменялась и теперь в ней появился параметр index. Скопируем новую ссылку для первой страницы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 1 # создаю переменную и с помощью форматирования строк добавляю ее в ссылку после index\n",
    "link = f'https://www.hse.ru/sci/diss/index{page}.html?spec=&chief=&fulltext=yes&author=&council=&degree_type=&type=1&year=2019'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.hse.ru/sci/diss/index1.html?spec=&chief=&fulltext=yes&author=&council=&degree_type=&type=1&year=2019'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "link # проверили, что работает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for page in range(1,7): \n",
    "    link = f'https://www.hse.ru/sci/diss/index{page}.html?spec=&chief=&fulltext=yes&author=&council=&degree_type=&type=1&year=2019'\n",
    "    soup = BeautifulSoup(requests.get(link).text, 'lxml')\n",
    "\n",
    "    links = []\n",
    "    for link in soup.find_all('a', text='Summary'):\n",
    "        links.append('https://www.hse.ru' + link.get('href'))\n",
    "\n",
    "    authors = []\n",
    "    for author in soup.find_all('td', text='Соискатель:'):\n",
    "        authors.append(author.parent()[1].get_text().split()[0])\n",
    "\n",
    "    for idx in range(len(authors)):\n",
    "        get_pdf(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готово!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
